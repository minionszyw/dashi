"""
LangChain AIå¯¹è¯æœåŠ¡
"""
import json
from typing import AsyncGenerator, Dict, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from sqlalchemy.orm import Session

from app.core.config import settings
from app.models.conversation import Conversation
from app.models.message import Message
from app.models.bazi_profile import BaziProfile
from app.prompts import SystemPromptManager


class LangChainChatService:
    """LangChainèŠå¤©æœåŠ¡"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=settings.OPENAI_MODEL,
            temperature=settings.OPENAI_TEMPERATURE,
            max_tokens=settings.OPENAI_MAX_TOKENS,
            streaming=True,
            openai_api_base=settings.OPENAI_BASE_URL,
            openai_api_key=settings.OPENAI_API_KEY
        )
    
    def _build_system_prompt(
        self,
        ai_style: str = "professional",
        bazi_info: Optional[Dict] = None
    ) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯
        
        Args:
            ai_style: AIå¯¹è¯é£æ ¼ (simple/balanced/professional)
            bazi_info: å…«å­—æ¡£æ¡ˆä¿¡æ¯ï¼ˆåŒ…å«name, genderå’Œbazi_resultä¸­çš„æ•°æ®ï¼‰
            
        Returns:
            å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
        """
        return SystemPromptManager.build_system_prompt(
            ai_style=ai_style,
            bazi_info=bazi_info
        )
    
    def _load_context_messages(
        self,
        conversation_id: str,
        context_size: int,
        db: Session
    ) -> list:
        """åŠ è½½ä¸Šä¸‹æ–‡æ¶ˆæ¯"""
        messages = db.query(Message).filter(
            Message.conversation_id == conversation_id
        ).order_by(Message.created_at.desc()).limit(context_size).all()
        
        # è½¬æ¢ä¸ºLangChainæ¶ˆæ¯æ ¼å¼
        context = []
        for msg in reversed(messages):
            if msg.role == "user":
                context.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                context.append(AIMessage(content=msg.content))
        
        return context
    
    async def stream_chat(
        self,
        user_message: str,
        conversation_id: str,
        user_id: str,
        db: Session
    ) -> AsyncGenerator[Dict, None]:
        """
        æµå¼å¯¹è¯
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_id: ä¼šè¯ID
            user_id: ç”¨æˆ·ID
            db: æ•°æ®åº“ä¼šè¯
            
        Yields:
            æ¶ˆæ¯å—å­—å…¸
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # è·å–ä¼šè¯ä¿¡æ¯
        conversation = db.query(Conversation).filter(
            Conversation.id == conversation_id
        ).first()
        
        logger.info(f"ğŸ“Œ ä¼šè¯ID: {conversation_id}")
        logger.info(f"ğŸ¨ å¯¹è¯æ¨¡å¼: {conversation.ai_style}")
        logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡æ¡æ•°: {conversation.context_size}")
        logger.info(f"ğŸ”— å…³è”å…«å­—æ¡£æ¡ˆID: {conversation.bazi_profile_id}")
        
        # è·å–AIå¯¹è¯é£æ ¼
        ai_style = conversation.ai_style or "professional"
        
        # è·å–å…«å­—ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        bazi_info = None
        if conversation.bazi_profile_id:
            logger.info(f"ğŸ” æ­£åœ¨æŸ¥è¯¢å…«å­—æ¡£æ¡ˆ: {conversation.bazi_profile_id}")
            profile = db.query(BaziProfile).filter(
                BaziProfile.id == conversation.bazi_profile_id
            ).first()
            
            if profile:
                # ç»„åˆå…«å­—æ¡£æ¡ˆçš„å®Œæ•´ä¿¡æ¯
                bazi_info = {
                    'name': profile.name,
                    'gender': profile.gender,
                    # ä»bazi_resultå­—æ®µè·å–å…«å­—åˆ†ææ•°æ®
                    **(profile.bazi_result or {})
                }
                logger.info(f"âœ… å…«å­—æ¡£æ¡ˆåŠ è½½æˆåŠŸ: {profile.name} ({profile.gender})")
                logger.info(f"   å…«å­—å­—æ®µ: {list(profile.bazi_result.keys()) if profile.bazi_result else 'æ— '}")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å…«å­—æ¡£æ¡ˆ: {conversation.bazi_profile_id}")
        else:
            logger.info("â„¹ï¸ å½“å‰ä¼šè¯æœªå…³è”å…«å­—æ¡£æ¡ˆ")
        
        # æ„å»ºç³»ç»Ÿæç¤ºï¼ˆä¼ å…¥ai_styleå’Œå®Œæ•´çš„bazi_infoï¼‰
        system_prompt = self._build_system_prompt(
            ai_style=ai_style,
            bazi_info=bazi_info
        )
        
        # åŠ è½½ä¸Šä¸‹æ–‡
        context_messages = self._load_context_messages(
            conversation_id,
            conversation.context_size,
            db
        )
        
        # æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨
        messages = [SystemMessage(content=system_prompt)]
        messages.extend(context_messages)
        messages.append(HumanMessage(content=user_message))
        
        # æµå¼ç”Ÿæˆå“åº”
        full_response = ""
        token_count = 0
        
        try:
            async for chunk in self.llm.astream(messages):
                content = chunk.content
                if content:
                    full_response += content
                    token_count += 1
                    
                    yield {
                        "type": "token",
                        "content": content,
                        "token_cost": token_count
                    }
        
        except Exception as e:
            yield {
                "type": "error",
                "message": f"AIå¯¹è¯å¤±è´¥ï¼š{str(e)}"
            }

